<!DOCTYPE html><html><head><title>SOP</title><meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" /><meta http-equiv="Pragma" content="no-cache" /><meta http-equiv="Expires" content="0" /><link rel = "stylesheet" type = "text/css" href = "https://hychn.github.io/style.css" /><meta name="viewport" content="width=device-width, initial-scale=1" /><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://hychn.github.io/tbl_contents.js"></script> </head> <body style="background-color:white;"> <div class="top"><button onclick="toggle_show()">Table of Contents</button> <button onclick="toggle_show_spoiler()">Toggle Spoiler</button>  <button onclick="toggle_hide_spoiler()">Toggle Hide</button> <div id="toc_frame"> <div id="toc"></div> </div> </div>   <div id="contents">
<h1>PURPOSE =====================================================================================</h1>

<ul>
<li>General questions and simple, natural ideas that form the answer; these things fascinate and bring me great joy. Here are two such examples I encountered in the industry.</li>
<li>First, while classifying and segmenting satellite images, I explored the simple ideas: Image processing kernels that modifies (blur, outline, dilate) images, gradient descent to adjust parameters of the kernels, nonlinear-activation functions. It was very satisfying seeing such simple ideas work together to classify large areas of terrain accurately and much faster than annotation-by-hand. </li>
<li><p>Second, when handling complex input data with intrinsic structure, I encoded the input to a latent space. Then, the encoded input was transformed with respect to the intrinsic structure. These ideas allowed the algorithm to handle the complex input much more efficiently and observe the 3D robotic-arm’s and 3D protein’s structural properties when the input was a 2D image or a 1D protein sequence.</p></li>
<li><p>How do you optimize a parameter that has no derivative? Does an optimum even exist? I encountered this key question many times while finding: optimal model architecture for predicting satellite images, best loss function for protein structure prediction, effective estimations of cost-to-go functions.</p></li>
<li>Search and estimation by sampling are good, natural ideas that could answer this question. However, I was not satisfied with the naive and impractical exhaustive sampling. It took so long! I wish to study this key question in more detail with more specific questions: What different types of sampling and estimation can we form? What is the consequence of using each? Are some better than others? Can we measure the quality of the estimation? What are the meaningful conditions/situations, where stronger conclusions can emerge?</li>
</ul>

<h1>WHY THIS PROGRAM =====================================================================================</h1>

<ul>
<li>UIUC CS PhD program's three key characteristics make it my top choice.</li>
<li>First, I am very interested in the theoretical RL research by Professor Nan Jiang. His papers, 'Towards Hyperparameter-free Policy Selection for Offline Reinforcement Learning' and 'A Few Expert Queries Suffices for Sample-Efficient RL with Resets and Linear Value Approximation' provides ideas related to my key questions of optimizing parameters without a derivative and sampling effectively.</li>
<li>Second, the course, 'Statistical Reinforcement Learning', will give me a solid background knowledge and jumpstart my research when I complete the novel theoretical research final project.</li>
<li>Finally, I appreciate the guidance of the Artificial Intelligence Qualifying Exam Committee where I will present my research complemented with a background material tutorial. I am heart-warmed by the committee's intent to encourage learning of fundamentals and diagnose weakness in my knowledge.</li>
</ul>

<h1>WHY YOU ARE QUALIFIED ==============================================================================</h1>

<ul>
<li><p>I know that the program will be a challenging and require me to grow beyond my comfort zone. My experiences of overcoming difficulties in academics, industry, and self-study makes me well-prepared.</p></li>
<li><p>Completing the master's degree in Applied Mathematics and Computer Science gave me experience of applying logical and theoretical concepts. Courses such as Linear Algebra, Algebra, Analysis, Operations Research provide a solid background to understand AI ideas at a fundamental level. Analyzing protein structure using CNNs and persistent homology in my thesis, gave me interdisciplinary insight on how math and CS complement each other. Finally, I really enjoyed the many hours thinking, meditating, and exploring theoretical concepts and questions.</p></li>
<li><p>Working in the industry gave me valuable experience overcoming difficult problems and producing consistent results. Here are three examples.</p>

<ul>
<li>First, working on segmentation of satellite images gave me the opportunity to read, implement from scratch, and benchmark papers such as U-Net and Deep Aggregate Net. The prototype and data analysis won a major government contract for our company.</li>
<li>Second, while developing a semiconductor-robot monitoring prototype for SK Hynix, I combined CNN, Novel View Synthesis, and pose estimation to track and pose the robot arm. From a complex semiconductor monitoring video, the prototype extracted several key details: the positions of the robotic arm, the various nozzles attached to the arm, when the nozzle was spraying a solution or not.</li>
<li>Finally, predicting 3D structures from 1D protein sequences gave me the experience of managing and finishing a complex project. I combined various neural network architectures such as, LSTM, CNNs, Attention, and ESM protein language embeddings into a single pipeline. Then I researched and experimented with distance, dihedral angle, frame-aligned point error functions. The resulting algorithm was much more stable, consistent, and quickly converging compared to starting baseline, Recurrent Geometric Networks. Over these three examples, I gained valuable insights that helped me break through difficult challenges: have cautious optimism that a nice-simple solution exists, focus on key-measurable objectives, and work at a steady-consistent pace.</li>
</ul></li>
<li><p>Pursuing challenging independent research interests during my self-studies taught me discipline, motivation and persistence. Studying the AlphaGo Zero paper, I implemented a Monte Carlo Search Tree model-based RL to play variations of tic-tac-toe with longer length-to-win and boards. It was very challenging to get the initial prototypes to work on larger variations of the game; I carefully and patiently tried many different ideas. Eventually, reasonable success was found using: multi-step lookahead using model value estimation, augmentation of board inputs using rigid transformations, adjustments on rollout sample size depending on game depth, and many minor optimizations. Wanting to understand the RL the much better, I am currently studying the fundamentals of markov decision process programming and optimal control.</p></li>
</ul>

<h1>CLOSING NARRATIVE =====================================================================================</h1>

<ul>
<li>AI is becoming more important and widely-used in the industry. By asking intriguing questions and finding simple, natural answers, I hope to contribute towards a more goal-aware and meaningful direction in AI research. Doing this brings great joy and satisfaction in my heart.</li>
</ul>

</div></body> </html>
