<!DOCTYPE html><html><head><title>RL</title><meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" /><meta http-equiv="Pragma" content="no-cache" /><meta http-equiv="Expires" content="0" /><link rel = "stylesheet" type = "text/css" href = "https://hychn.github.io/style.css" /><meta name="viewport" content="width=device-width, initial-scale=1" /><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://hychn.github.io/tbl_contents.js"></script> </head> <body style="background-color:white;"> <div class="top"><button onclick="toggle_show()">Table of Contents</button> <button onclick="toggle_show_spoiler()">Toggle Spoiler</button>  <button onclick="scroll_bottom()">GOTO END</button> <div id="toc_frame"> <div id="toc"></div> </div> </div>   <div id="contents">
<h1>Multi-hop networks</h1>

<ul>
<li>Traffic engineering (TE) enables optimal forwarding and routing rules to meet quality of service (QoS) requirements for a large volume of traffic flows</li>
<li>Learna stochastic routing policy for each router so that each router can send a packet to the next-hop router according to the learned  optimal prob</li>
<li>Naturally leads to multi-path TE strategies, to minimize E2E delay
<ul>
<li>heuristic approach</li>
<li>model-based optimization</li>
<li>model-free optimiation</li>
</ul></li>
<li>Model-based optimization based on network utility maximization (NUM framework) where TE problem is formulated as a constrained maximization problems of the utility function</li>
<li>Limitations of model-based approachhes, Multi-agent reinforcement learning (MARL) developement of model-free distributed TE optimization schemes [7-10] where each router, acting as an agent learns the optimal local TE policy in such a way that the colective TE policy of all routers can achieve the optimal E2E TE performance</li>
<li>Mainly focuses on applying action-value methods such as Q-learning and its variants [11-15 which maximizes TE objective the E2E delay</li>
<li>The paper proposes a stochastic policy gradient RL
<ul>
<li>Formulate the problem as a multi-path TE problem as a multi-agent Markov decision process (MA-MDP)</li>
<li>MA-MDP problem, emply the multi-agent actor-critic algorithm where each router has its own actor and critic</li>
</ul></li>
</ul>

<h1>Network Planning with Deep Reinforcement Learning</h1>

<ul>
<li>Networking planning, formulated as Integer Linear Programming problem (ILP)</li>
<li>Uses GNN</li>
<li><p>Use RL to prune search space and the use ILP solver to find optimal solution</p></li>
<li><p>Netowkring planning is a hard combinatorial optimzation problem</p></li>
<li>Cross-layer decisions taht involve both the IP layer and optical layer needs to be made.</li>
<li>Planned network must satisfy certain service expectations specified by the operator</li>
<li>Converging optimal solution is hard with RL</li>
</ul>

</div></body> </html>
