<!DOCTYPE html><html><head><title>Title</title><meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" /><meta http-equiv="Pragma" content="no-cache" /><meta http-equiv="Expires" content="0" /><link rel = "stylesheet" type = "text/css" href = "https://hychn.github.io/style.css" /><meta name="viewport" content="width=device-width, initial-scale=1" /><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://hychn.github.io/tbl_contents.js"></script> </head> <body style="background-color:white;"> <div class="top"><button onclick="toggle_show()">Table of Contents</button> <button onclick="toggle_show_spoiler()">Toggle Spoiler</button>  <button onclick="scroll_bottom()">GOTO END</button> <div id="toc_frame"> <div id="toc"></div> </div> </div>   <div id="contents">
<p>RAPID UNDERSTANDING AND BOND NEUROPLASCITY FORMING
  * repeadtly look at it, re look at it (multiple things at once)
Follow the guide of openai, dont fall down too many rabbit hole
Why does model only seek to optimize it's ability to win and not defend?</p>

<h1>Introduction</h1>

<ul>
<li>PART 1: a short introduction to RL terminology, kinds of algorithms, and basic theory,</li>
<li><a href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html">Key Papers in DeepRL</a></li>
<li><a href="https://github.com/openai/spinningup">Code repo of key algo</a></li>
<li><a href="https://spinningup.openai.com/en/latest/spinningup/exercises.html">Excercises</a></li>
</ul>

<h2>ESSAY: Spinning Up as a Deep RL Researcher: https://spinningup.openai.com/en/latest/spinningup/spinningup.html</h2>

<blockquote onclick="click_spoiler(event)" class="spoiler">
  <h3>The Right Background</h3>
  
  <ul>
  <li>Mathematical background 
  <ul>
  <li><strong>random variables, Bayes theorem, chain rule prob, expected value, standard dev, importance sampling</strong></li>
  </ul></li>
  <li>RL main concepts and terminology
  <ul>
  <li><strong>states, actions, trajectories, policies, reward, value functions, action-value functions</strong></li>
  <li>[RL Intro] (https://github.com/jachiam/rl-intro/blob/master/Presentation/rl_intro.pdf)</li>
  <li>[Overview] https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html</li>
  <li><a href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">Classical RL algorithms</a></li>
  </ul></li>
  <li>Learn by Doing
  <ul>
  <li>core deep RL algos Simplicity</li>
  </ul></li>
  <li>simplest/smallest implementation first
  <ul>
  <li><a href="https://arxiv.org/abs/1604.06778">Vanilla policy gradient REINFORCE</a></li>
  <li><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DQN</a></li>
  <li><a href="https://blog.openai.com/baselines-acktr-a2c/">A2C</a></li>
  <li><a href="https://arxiv.org/abs/1707.06347">PPO</a></li>
  <li><a href="https://arxiv.org/abs/1509.02971">DDPG</a></li>
  <li>Focus on understanding: 
  <ul>
  <li>writing working RL code requires clear, detail-oriented understanding of the algorithms.</li>
  <li>This is because broken RL code almost always fails silently.</li>
  </ul></li>
  <li>ablations, intuition for parameters or subroutines have the biggest impact</li>
  <li>Don't overfit</li>
  <li>Iterate fast in simple environments: CartPole-v0, InvertedPendulum-v0, FrozenLake-v0, HalfCheetah-v2 (100-250 steps) OpenAI Gym
  <ul>
  <li>Simplest possible toy task</li>
  </ul></li>
  <li>If it doesn't work, assume there is a bug </li>
  <li>Measure everything, replay of agent's performance now and then</li>
  <li>Scale experiments when things work
  ### Developing a Research Project</li>
  <li>Avoid reinventing the wheel
  ### Doing Rigorous Research in RL
  ### Closing Thoughts
  ### PS: Other Resources</li>
  </ul></li>
  </ul>
</blockquote>

<h2>Installation</h2>

<ul>
<li>OpenMPI?</li>
<li>Spinning Up?</li>
</ul>

<h2>Algorithms</h2>

<ul>
<li>Vanilla Policy Gradient (VPG)</li>
<li>Trust Region Policy Optimization (TRPO)</li>
<li>Deep Deterministic Policy Gradient (DDPG)</li>
<li>Twin Delayed DDPG (TD3)</li>
<li>Proximal Policy Optimization (PPO)</li>
<li>Soft Actor-Critic (SAC)</li>
</ul>

<h3>On-Policy Algorithms</h3>

<ul>
<li>VPG -> TRPO -> PPO</li>
<li>on-policy: 
<ul>
<li>don't use old data, weaker on sample efficiency</li>
<li>directly optimize on bjective, policy performance</li>
<li>trades stability  for sample efficiency</li>
</ul></li>
</ul>

<h3>Off-Policy Algorithms</h3>

<ul>
<li>Qlearning -> DDPG -> TD3 -> SAC
<ul>
<li>Bellman's equation for optimality (Q-function) can be trained to satisfy using any environment iteraction data, as long as there is enough experience from high-reward areas in environment)</li>
</ul></li>
<li>sample efficient if it can get the most out of every sample. Humans are very sample efficient. mcst0 is not it needs many examples to learn2play</li>
<li>Importance sampling is a technique to filter these samples</li>
</ul>

<h3>Code Format</h3>

<ul>
<li>algorithm: 
<ul>
<li>class definition for experience buffer object, used to store information from agent-environment interactions</li>
<li>Logger</li>
<li>Random seed setting</li>
<li>Constructing actor-critic Pytorch module via the actor_critic function passed to algorithm function</li>
<li>Instantiate enviroment/experience buffer</li>
<li>set up loss function/ diagnostic</li>
<li>main loop
<ul>
<li>run agent in environment</li>
<li>update parameters of agent</li>
<li>log key performance metrics and save</li>
</ul></li>
</ul></li>
</ul>

<h2>Part 1: Key Concepts in RL</h2>

<ul>
<li>action space: discrete/continuous</li>
<li>policy deterministic \(a_t = \mu_\theta(s_t)\) stochastic \(a_t ~ \pi_\theta(.|s_t)\)</li>
<li>Deterministic Policies</li>
<li><p>Stochastic Policies</p>

<ul>
<li>Categorical policy (discrete action space) , similar to Categorical NN model</li>
<li>Diagonal Guassian policy (continuous action space)
<ul>
<li><strong>multivariate diagonal distribution</strong></li>
<li>where the <strong>cross variance</strong> is a diagonal matrix</li>
<li>described by mean vector \(\mu\) and convariance matrix \(\sigma\)</li>
</ul></li>
</ul></li>
<li><p>sampling actions from policy</p></li>
<li><p>computing log likelihoods of particular actions \(log \pi_\theta (a|s)\)</p></li>







<li><p>mcst0 doesn't seem to see a move that leads to immediate loss afterwards</p>

<ul>
<li>which it should if it follows P directly, since our network can consistent make the terminal move</li>
<li>this indicates somethig is wrong with the preceding step, Nx(MCST search) -> the branching/ubc</li>
<li>maybe the random prob is too much for the numerical stability to apply?</li>
</ul></li>
</ul>

<h1>Opportunities</h1>

<ul>
<li>Open AI Scholars https://openai.com/blog/openai-scholars/ scholars@openai.com</li>
<li><p>Nuts and Bolts of deep rl research: http://joschu.net/docs/nuts-and-bolts.pdf</p>

<h1>Future (Notable sights)</h1></li>
<li><p>monotonic improvement theory (basis of advanced policy gradient algorithms)? http://joschu.net/docs/thesis.pdf</p></li>
</ul>

</div></body> </html>
